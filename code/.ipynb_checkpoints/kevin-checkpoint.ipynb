{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ad6953",
   "metadata": {},
   "source": [
    "## Thank ye, good Sir  ðŸŽ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyndl import preprocess, count, ndl\n",
    "from tqdm.notebook import tqdm\n",
    "from dp.phonemizer import Phonemizer\n",
    "\n",
    "phonemizer = Phonemizer.from_checkpoint('../data/en_us_cmudict_forward.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f1d47",
   "metadata": {},
   "source": [
    "#### Make first version of event file with pyndl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15884e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(os.listdir('../data/subtitles/')[:10000]):\n",
    "    preprocess.create_event_file(corpus_file = '../data/subtitles/' + file ,\n",
    "                             event_file = '../data/individual_eventfiles/' + file + '.gz',\n",
    "                             symbols = \"a-zA-Z'\",\n",
    "                             context_structure ='document',\n",
    "                             event_structure = 'word_to_word',\n",
    "                             event_options = (1,0),\n",
    "                             lower_case = True,\n",
    "                             cue_structure = 'word_to_word')\n",
    "# All subtitle files are 446283. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d1f43",
   "metadata": {},
   "source": [
    "#### Add syllable, context and segment cues with cue tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Syllabifier script. \n",
    "# English language settings for the language parameter in the syllabifier.\n",
    "English = {\n",
    "    'consonants': ['B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L',\n",
    "                   'M', 'N', 'NG', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W',\n",
    "                   'Y', 'Z', 'ZH'],\n",
    "    'vowels': [ 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH',\n",
    "               'IY', 'OW', 'OY', 'UH', 'UW'],\n",
    "    'onsets': ['P', 'T', 'K', 'B', 'D', 'G', 'F', 'V', 'TH', 'DH', 'S', 'Z',\n",
    "               'SH', 'CH', 'JH', 'M', 'N', 'R', 'L', 'HH', 'W', 'Y', 'P R',\n",
    "               'T R', 'K R', 'B R', 'D R', 'G R', 'F R', 'TH R', 'SH R',\n",
    "               'P L', 'K L', 'B L', 'G L', 'F L', 'S L', 'T W', 'K W',\n",
    "               'D W','S W', 'S P', 'S T', 'S K', 'S F', 'S M', 'S N', 'G W',\n",
    "               'SH W', 'S P R', 'S P L', 'S T R', 'S K R', 'S K W', 'S K L',\n",
    "               'TH W', 'ZH', 'P Y', 'K Y', 'B Y', 'F Y', 'HH Y', 'V Y',\n",
    "               'TH Y', 'M Y', 'S P Y', 'S K Y', 'G Y', 'HH W', '']\n",
    "    }\n",
    "     \n",
    "def syllabify(language, word):\n",
    "    '''Syllabifies the word, given a language configuration loaded with\n",
    "    loadLanguage. word is either a string of phonemes from the CMU\n",
    "    pronouncing dictionary set (with optional stress numbers after vowels),\n",
    "    or a Python list of phonemes, e.g. \"B AE1 T\" or [\"B\", \"AE1\", \"T\"]\n",
    "    '''\n",
    "\n",
    "    if type(word) == str:\n",
    "        word = word.split()\n",
    "    # This is the returned data structure.\n",
    "    syllables = []\n",
    "\n",
    "    # This maintains a list of phonemes between nuclei.\n",
    "    internuclei = []\n",
    "\n",
    "    for phoneme in word :\n",
    "\n",
    "        phoneme = phoneme.strip()\n",
    "        if phoneme == \"\" :\n",
    "            continue\n",
    "        stress = None\n",
    "        if phoneme[-1].isdigit() :\n",
    "            stress = int(phoneme[-1])\n",
    "            phoneme = phoneme[0:-1]\n",
    "\n",
    "        # Split the consonants seen since the last nucleus into coda and\n",
    "        # onset.\n",
    "        if phoneme in language[\"vowels\"] :\n",
    "\n",
    "            coda = None\n",
    "            onset = None\n",
    "\n",
    "            # If there is a period in the input, split there.\n",
    "            if \".\" in internuclei :\n",
    "                period = internuclei.index(\".\")\n",
    "                coda = internuclei[:period]\n",
    "                onset = internuclei[period+1:]\n",
    "\n",
    "            else :\n",
    "                # Make the largest onset we can. The 'split' variable marks\n",
    "                # the break point.\n",
    "                for split in range(0, len(internuclei)+1) :\n",
    "                    coda = internuclei[:split]\n",
    "                    onset = internuclei[split:]\n",
    "\n",
    "                    # If we are looking at a valid onset, or if we're at the\n",
    "                    # start of the word (in which case an invalid onset is\n",
    "                    # better than a coda that doesn't follow a nucleus), or\n",
    "                    # if we've gone through all of the onsets and we didn't\n",
    "                    # find any that are valid, then split the nonvowels\n",
    "                    # we've seen at this location.\n",
    "                    if \" \".join(onset) in language[\"onsets\"] \\\n",
    "                       or len(syllables) == 0 \\\n",
    "                       or len(onset) == 0 :\n",
    "                       break\n",
    "\n",
    "\n",
    "            # Tack the coda onto the coda of the last syllable. Can't do it\n",
    "            # if this is the first syllable.\n",
    "            if len(syllables) > 0 :\n",
    "                syllables[-1][3].extend(coda)\n",
    "\n",
    "            # Make a new syllable out of the onset and nucleus.\n",
    "            syllables.append( (stress, onset, [phoneme], []) )\n",
    "\n",
    "            # At this point we've processed the internuclei list.\n",
    "            internuclei = []\n",
    "\n",
    "        elif not phoneme in language[\"consonants\"] and phoneme != \".\" :\n",
    "            raise ValueError(\"Invalid phoneme: \" + phoneme)\n",
    "\n",
    "        else : # a consonant\n",
    "            internuclei.append(phoneme)\n",
    "\n",
    "    # Done looping through phonemes. We may have consonants left at the end.\n",
    "    # We may have even not found a nucleus.\n",
    "    if len(internuclei) > 0 :\n",
    "        if len(syllables) == 0 :\n",
    "            syllables.append( (None, internuclei, [], []) )\n",
    "        else :\n",
    "            syllables[-1][3].extend(internuclei)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "def stringify(syllables) :\n",
    "    '''This function takes a syllabification returned by syllabify and\n",
    "       turns it into a string, with phonemes spearated by spaces and\n",
    "       syllables spearated by periods.'''\n",
    "    ret = []\n",
    "    for syl in syllables :\n",
    "        stress, onset, nucleus, coda = syl\n",
    "        if stress != None and len(nucleus) != 0 :\n",
    "            nucleus[0] += str(stress)\n",
    "        ret.append(\"\".join(onset + nucleus + coda))\n",
    "    return \" \".join(ret)\n",
    "\n",
    "language = English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting/formatting segments/syllables\n",
    "def get_segments(word, upper = False):\n",
    "    \"\"\"Returns the segments of a word.\"\"\"\n",
    "    raw_segment_string = phonemizer(word, lang='en_us')\n",
    "    \n",
    "    if upper:\n",
    "        segments_string = re.sub(r'[\\[\\]-]',' ', raw_segment_string)\n",
    "        segments = segments_string.split()\n",
    "        return segments\n",
    "    else:\n",
    "        segments_string = re.sub(r'[\\[\\]-]',' ', raw_segment_string.lower())\n",
    "        segments = segments_string.split()\n",
    "        return segments\n",
    "\n",
    "def join_segments(word):\n",
    "    \"\"\"Returns the segments of a word in a cue formatted string.\"\"\"\n",
    "    segments = get_segments(word)\n",
    "    segments_y = []\n",
    "    for segment in segments:\n",
    "        segment = 's.' + segment\n",
    "        segments_y.append(segment)\n",
    "    segments_joined = '_'.join(segments_y)\n",
    "    return segments_joined\n",
    "\n",
    "def join_syllables(syllables):\n",
    "    \"\"\"Returns the syllables of a word in a cue formatted string.\"\"\"\n",
    "    syll_list = syllables.split()\n",
    "    syllable_cuestring = []\n",
    "    for entry in syll_list:\n",
    "        syllable_cue = 'y.' + entry\n",
    "        syllable_cuestring.append(syllable_cue.lower())\n",
    "    syllables_joined = '_'.join(syllable_cuestring)\n",
    "    return syllables_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2aa15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = {}\n",
    "\n",
    "for file in tqdm(os.listdir('../data/individual_eventfiles/')):\n",
    "    df = pd.read_csv('../data/individual_eventfiles/' + file, \n",
    "               sep = '\\t', \n",
    "               low_memory = True, \n",
    "               dtype = ({'outcomes':'category'}), \n",
    "               engine = 'c')\n",
    "    words = df['outcomes'].tolist()\n",
    "\n",
    "    for index, word in enumerate(words):\n",
    "\n",
    "        # Get the CONTEXT (preceding and following word).\n",
    "        if index == 0:\n",
    "            context = 'c.' + words[index+1]\n",
    "        elif index == len(words)-1:\n",
    "            context = 'c.' + words[index-1]\n",
    "        else:\n",
    "            previous_word = 'c.' + words[index-1]\n",
    "            following_word = 'c.' + words[index+1]\n",
    "            context = previous_word + '_' + following_word \n",
    "\n",
    "        # Get the SEGMENTS of the word. Transcription dict entry is cue-string formatted segments for the event file\n",
    "        # and unformatted segments for the syllabify script. \n",
    "        if word not in transcriptions.keys(): \n",
    "            cue_segments = join_segments(word)\n",
    "            segments = get_segments(word, upper = True)\n",
    "            transcriptions[word] = {'cue_segments': cue_segments, 'segments': segments} \n",
    "        else:\n",
    "            cue_segments = transcriptions[word]['cue_segments']\n",
    "\n",
    "        # Get the SYLLABLES of the word. \n",
    "        raw_syllables = stringify(syllabify(English, transcriptions[word]['segments']))\n",
    "        syllables = join_syllables(raw_syllables)\n",
    "\n",
    "        cues = context + '_' + syllables + '_' + cue_segments\n",
    "\n",
    "        # Append all information to the dataframe. \n",
    "        df.at[index, 'cues'] = cues\n",
    "\n",
    "    # Save eventfile. \n",
    "    df.to_csv('../data/individual_eventfiles/' + file, sep = '\\t', index = False, compression = 'gzip')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9f798",
   "metadata": {},
   "source": [
    "#### Make batches, 2k files each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ed6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "dataframes = []\n",
    "batch_num = 1\n",
    "\n",
    "for file in tqdm(os.listdir('../data/individual_eventfiles/')):\n",
    "    if len(dataframes) < 2000:\n",
    "        files.append(file.split(\".\")[0])\n",
    "        df = pd.read_csv('../data/individual_eventfiles/' + file,\n",
    "                         sep = '\\t', low_memory = True, \n",
    "                         dtype = ({'outcomes':'category'}), \n",
    "                         engine = 'c')\n",
    "        dataframes.append(df)\n",
    "        \n",
    "    else: \n",
    "        # Write filenames of current batch to json.\n",
    "        with open('../data/logs/batch' + str(batch_num) + '.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(files, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # Save the 2000 files in one batch. \n",
    "        batch = pd.concat(files)\n",
    "        batch.to_csv('../data/batchfiles/batch' + str(batch_num) + '.gz', sep = '\\t', index = False, compression = 'gzip')\n",
    "\n",
    "            \n",
    "        batch_num =+1    \n",
    "        files = []    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e68f2",
   "metadata": {},
   "source": [
    "#### Replace the context cues below the frequency cutoff with unkown and remove them as outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_cutoff = []\n",
    "drop_outcomes = []\n",
    "\n",
    "for file in tqdm(os.listdir('../data/batchfiles/')):\n",
    "    df = pd.read_csv('../data/batchfiles/' + file,\n",
    "                     sep = '\\t', \n",
    "                     low_memory = True, \n",
    "                     dtype = ({'outcomes':'category'}), \n",
    "                     engine = 'c')\n",
    "    words = df['outcomes'].tolist()\n",
    "    \n",
    "    # Look through outcomes for below cutoff words. If found, replace cues in the line before and after with unkown\n",
    "    for index, word in enumerate(words):\n",
    "        if word in below_cutoff:\n",
    "            if indx != 0:\n",
    "                df.at[indx-1, 'cues'] = df.at[indx-1, 'cues'].replace('c.' + str(word), '<unknown>')\n",
    "            if indx != len(df)-1:\n",
    "                df.at[indx+1, 'cues'] = df.at[indx+1, 'cues'].replace('c.' + str(word), '<unknown>')\n",
    "            \n",
    "            # Append index of outcome so it can be removed later. Otherwise the index changes. \n",
    "            drop_outcomes.append(index)\n",
    "            \n",
    "    df.drop(index=sammlung, inplace = True)  # Save memory with inplace \n",
    "    df.to_csv('../data/batchfiles/' + file, sep = '\\t', index = False, compression = 'gzip')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d58405",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ndl.ndl(events = '../data/final_eventfile_buckeye.gz', \n",
    "                  alpha = 0.1, \n",
    "                  betas = (0.1,0.1), \n",
    "                  lambda_=1.0, \n",
    "                  method='openmp', \n",
    "                  remove_duplicates=True, \n",
    "                  verbose=True)\n",
    "\n",
    "weights.to_netcdf('../output/weights/' + 'weights_buckeye.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24116948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous weights \n",
    "previous_weights = xr.open_dataarray('../output/weights/weights_buckeye.nc')\n",
    "\n",
    "weights = ndl.ndl(events='../data/batchfiles/batch1.gz', \n",
    "                  alpha=0.1, \n",
    "                  betas=(0.1, 0.1), \n",
    "                  method='openmp', \n",
    "                  verbose = True, \n",
    "                  weights = previous_weights, \n",
    "                  remove_duplicates = False)\n",
    "\n",
    "weights.to_netcdf('../output/weights/weights_buckeyeNb1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa231f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_weights = xr.open_dataarray('../output/weights/weights_buckeyeNb1.nc')\n",
    "\n",
    "weights = ndl.ndl(events='../data/batchfiles/batch2.gz', \n",
    "                  alpha=0.1, \n",
    "                  betas=(0.1, 0.1), \n",
    "                  method='openmp', \n",
    "                  verbose = True, \n",
    "                  weights = previous_weights, \n",
    "                  remove_duplicates = False)\n",
    "\n",
    "weights.to_netcdf('../output/weights/weights_buckeyeNb1Nb2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_weights = xr.open_dataarray('../output/weights/weights_buckeyeNb1Nb2.nc')\n",
    "\n",
    "weights = ndl.ndl(events='../data/batchfiles/batch3.gz', \n",
    "                  alpha=0.1, \n",
    "                  betas=(0.1, 0.1), \n",
    "                  method='openmp', \n",
    "                  verbose = True, \n",
    "                  weights = previous_weights, \n",
    "                  remove_duplicates = False)\n",
    "\n",
    "weights.to_netcdf('../output/weights/weights_buckeyeNb1Nb2Nb3.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f439e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_weights = xr.open_dataarray('../output/weights/weights_buckeyeNb1Nb2Nb3.nc')\n",
    "\n",
    "weights = ndl.ndl(events='../data/batchfiles/batch4.gz', \n",
    "                  alpha=0.1, \n",
    "                  betas=(0.1, 0.1), \n",
    "                  method='openmp', \n",
    "                  verbose = True, \n",
    "                  weights = previous_weights, \n",
    "                  remove_duplicates = False)\n",
    "\n",
    "weights.to_netcdf('../output/weights/weights_buckeyeNb1Nb2Nb3Nb4.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
